# ============================================================================
# –†–ï–ö–£–†–†–ï–ù–¢–ù–ê–Ø –ù–ï–ô–†–û–ù–ù–ê–Ø –°–ï–¢–¨ (RNN/LSTM) –î–õ–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –§–ò–¢–ù–ï–° –ü–†–û–ì–†–ê–ú–ú
# ============================================================================

# ============================================================================
# –†–ê–ó–î–ï–õ 1: –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö
# ============================================================================
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# –î–ª—è Google Colab
from google.colab import files
import io
import joblib

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
np.random.seed(42)
tf.random.set_seed(42)

print("‚úì –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
print(f"TensorFlow –≤–µ—Ä—Å–∏—è: {tf.__version__}")

# ============================================================================
# –†–ê–ó–î–ï–õ 2: –°–û–ó–î–ê–ù–ò–ï –ò–õ–ò –ó–ê–ì–†–£–ó–ö–ê –î–ê–¢–ê–°–ï–¢–ê
# ============================================================================
print("\n" + "="*70)
print("–°–û–ó–î–ê–ù–ò–ï –ò–õ–ò –ó–ê–ì–†–£–ó–ö–ê –î–ê–¢–ê–°–ï–¢–ê")
print("="*70)

def create_synthetic_fitness_data(n_samples=5000):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∏—Ç–Ω–µ—Å-—Ç—Ä–µ–∫–∏–Ω–≥–∞"""
    np.random.seed(42)

    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–±–µ–∑ Member_ID, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å KeyError)
    data = {
        'Age': np.random.randint(18, 65, n_samples),
        'Gender': np.random.choice(['Male', 'Female'], n_samples),
        'Weight': np.random.normal(70, 15, n_samples).clip(40, 120),
        'Height': np.random.normal(170, 10, n_samples).clip(150, 200),
        'BMI': np.random.normal(24, 4, n_samples).clip(16, 40),
        'Workout_Hours': np.random.exponential(3, n_samples).clip(0, 10),
        'Workout_Type': np.random.choice(['Cardio', 'Strength', 'Mixed'], n_samples),
        'Calories_Burned': np.random.normal(400, 150, n_samples).clip(100, 1000),
        'Heart_Rate': np.random.normal(120, 20, n_samples).clip(60, 180),
        'Sleep_Hours': np.random.normal(7, 1.5, n_samples).clip(4, 12),
        'Stress_Level': np.random.randint(1, 10, n_samples),
        'Diet_Score': np.random.randint(1, 10, n_samples),
        'Previous_Experience': np.random.exponential(2, n_samples).clip(0, 10)
    }

    df = pd.DataFrame(data)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –æ–ø—ã—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    experience_score = (
        df['Previous_Experience'] * 0.3 +
        df['Workout_Hours'] * 0.2 +
        df['Calories_Burned'] * 0.15 +
        df['Diet_Score'] * 0.1 +
        df['Age'] * 0.05 +
        np.random.normal(0, 0.3, n_samples)
    )

    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∏ –∫–ª–∞—Å—Å–∞ —Å —á–µ—Ç–∫–∏–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏
    percentiles = np.percentile(experience_score, [33, 66])

    # –ù–∞—á–∏–Ω–∞—é—â–∏–π —É—Ä–æ–≤–µ–Ω—å (1)
    df['Experience_Level'] = 1

    # –°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å (2)
    mask_intermediate = (experience_score >= percentiles[0]) & (experience_score < percentiles[1])
    df.loc[mask_intermediate, 'Experience_Level'] = 2

    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å (3)
    mask_advanced = experience_score >= percentiles[1]
    df.loc[mask_advanced, 'Experience_Level'] = 3

    return df

# –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π
try:
    # –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–∞–π–ª, –∑–∞–≥—Ä—É–∂–∞–µ–º –µ–≥–æ
    df = pd.read_csv('/content/sample_data/gym_members_exercise_tracking.csv')
    print("‚úì –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
    required_columns = ['Age', 'Gender', 'Weight', 'Height', 'BMI', 'Workout_Hours',
                       'Workout_Type', 'Calories_Burned', 'Heart_Rate', 'Sleep_Hours',
                       'Stress_Level', 'Diet_Score', 'Previous_Experience', 'Experience_Level']

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã –µ—Å—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
    existing_columns = df.columns.tolist()
    missing_columns = [col for col in required_columns if col not in existing_columns]

    if missing_columns:
        print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–æ–ª–±—Ü—ã: {missing_columns}")
        print("–°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç...")
        df = create_synthetic_fitness_data(5000)
    else:
        print("‚úì –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç")

except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
    print("–°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç...")
    df = create_synthetic_fitness_data(5000)

print(f"\n–§–æ—Ä–º–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
print(f"–°—Ç–æ–ª–±—Ü—ã: {df.columns.tolist()}")
print("\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:")
print(df.head())
print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Experience_Level:")
print(df['Experience_Level'].value_counts().sort_index())
print(f"\n–ü—Ä–æ—Ü–µ–Ω—Ç—ã:")
print(df['Experience_Level'].value_counts(normalize=True).sort_index() * 100)

# ============================================================================
# –†–ê–ó–î–ï–õ 3: –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•
# ============================================================================
print("\n" + "="*70)
print("–ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•")
print("="*70)

# –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
print("\n1. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
print(f"   –î–æ –æ—á–∏—Å—Ç–∫–∏: {df.shape}")
df = df.dropna().reset_index(drop=True)
print(f"   –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {df.shape}")

# –ö–æ–¥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
print("\n2. –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö...")
categorical_cols = ['Gender', 'Workout_Type']
label_encoders = {}

for col in categorical_cols:
    if col in df.columns:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        label_encoders[col] = le
        print(f"   {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}")
    else:
        print(f"   ‚ö†Ô∏è –°—Ç–æ–ª–±–µ—Ü {col} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ")

# –†–∞–∑–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
print("\n3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π...")
X = df.drop('Experience_Level', axis=1).values
y = df['Experience_Level'].values

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º y –≤ 0-based –∏–Ω–¥–µ–∫—Å—ã [1,2,3] -> [0,1,2]
y = y - 1

print(f"   X shape: {X.shape}")
print(f"   y shape: {y.shape}")
print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã –≤ y: {np.unique(y)}")
print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.bincount(y)}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
class_counts = np.bincount(y)
if len(class_counts) == 3:
    print(f"   –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {class_counts[0]/len(y):.2%}, {class_counts[1]/len(y):.2%}, {class_counts[2]/len(y):.2%}")
else:
    print(f"   ‚ö†Ô∏è –ö–ª–∞—Å—Å—ã –Ω–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ")

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
print("\n4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
scaler = StandardScaler()
X = scaler.fit_transform(X)
print(f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
print(f"   –°—Ä–µ–¥–Ω–µ–µ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {X.mean():.4f}")
print(f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {X.std():.4f}")

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è LSTM
print("\n5. –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è LSTM...")
sequence_length = 5
n_features = X.shape[1]

# –ü—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
def create_sequences_simple(X, y, sequence_length=5):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø—Ä–æ—Å—Ç—ã–º —Å–ø–æ—Å–æ–±–æ–º"""
    X_seq = []
    y_seq = []

    for i in range(len(X) - sequence_length):
        X_seq.append(X[i:i+sequence_length])
        y_seq.append(y[i+sequence_length-1])  # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

    return np.array(X_seq), np.array(y_seq)

X_sequences, y_sequences = create_sequences_simple(X, y, sequence_length)

print(f"   –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {sequence_length}")
print(f"   X_sequences shape: {X_sequences.shape}")
print(f"   y_sequences shape: {y_sequences.shape}")
print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö: {np.bincount(y_sequences)}")

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä—ã
print("\n6. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä—ã...")
X_train, X_test, y_train, y_test = train_test_split(
    X_sequences, y_sequences,
    test_size=0.2,
    random_state=42,
    stratify=y_sequences
)

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train,
    test_size=0.2,
    random_state=42,
    stratify=y_train
)

print(f"   –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –Ω–∞–±–æ—Ä: {X_train.shape} ({(len(X_train)/len(X_sequences)*100):.1f}%)")
print(f"   –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä: {X_val.shape} ({(len(X_val)/len(X_sequences)*100):.1f}%)")
print(f"   –¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: {X_test.shape} ({(len(X_test)/len(X_sequences)*100):.1f}%)")
print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ —Ç—Ä–µ–π–Ω–µ: {np.bincount(y_train)}")
print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {np.bincount(y_val)}")
print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ —Ç–µ—Å—Ç–µ: {np.bincount(y_test)}")

# ============================================================================
# –†–ê–ó–î–ï–õ 4: –ü–û–°–¢–†–û–ï–ù–ò–ï –ú–û–î–ï–õ–ò LSTM
# ============================================================================
print("\n" + "="*70)
print("–ü–û–°–¢–†–û–ï–ù–ò–ï –ú–û–î–ï–õ–ò LSTM")
print("="*70)

# –°–±—Ä–æ—Å —Å–µ—Å—Å–∏–π TensorFlow –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
keras.backend.clear_session()

model = Sequential([
    # –ü–µ—Ä–≤—ã–π LSTM —Å–ª–æ–π
    layers.LSTM(
        128,
        return_sequences=True,
        input_shape=(sequence_length, n_features),
        kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),
        recurrent_regularizer=keras.regularizers.l2(1e-5),
        name='LSTM_1'
    ),
    layers.BatchNormalization(name='BatchNorm_1'),
    layers.Dropout(0.3, name='Dropout_1'),

    # –í—Ç–æ—Ä–æ–π LSTM —Å–ª–æ–π
    layers.LSTM(
        64,
        return_sequences=False,
        kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),
        name='LSTM_2'
    ),
    layers.BatchNormalization(name='BatchNorm_2'),
    layers.Dropout(0.3, name='Dropout_2'),

    # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
    layers.Dense(64, activation='relu', name='Dense_1',
                kernel_regularizer=keras.regularizers.l2(1e-4)),
    layers.BatchNormalization(name='BatchNorm_3'),
    layers.Dropout(0.2, name='Dropout_3'),

    layers.Dense(32, activation='relu', name='Dense_2'),
    layers.BatchNormalization(name='BatchNorm_4'),

    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (3 –∫–ª–∞—Å—Å–∞)
    layers.Dense(3, activation='softmax', name='Output')
])

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
print("\n–ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏...")
optimizer = Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07
)

model.compile(
    optimizer=optimizer,
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print("‚úì –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

# –í—ã–≤–æ–¥ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏
print("\n–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
model.summary()

# ============================================================================
# –†–ê–ó–î–ï–õ 5: –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï CALLBACKS
# ============================================================================
print("\n" + "="*70)
print("–û–ü–†–ï–î–ï–õ–ï–ù–ò–ï CALLBACKS")
print("="*70)

early_stopping = EarlyStopping(
    monitor='val_accuracy',
    patience=15,
    restore_best_weights=True,
    verbose=1,
    mode='max',
    min_delta=0.001
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1,
    mode='min'
)

checkpoint = ModelCheckpoint(
    'best_lstm_fitness_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1,
    mode='max'
)

callbacks = [early_stopping, reduce_lr, checkpoint]

print("‚úì Callbacks —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã:")
print("  - Early Stopping (patience=15)")
print("  - Learning Rate Reduction (factor=0.5)")
print("  - Model Checkpoint")

# ============================================================================
# –†–ê–ó–î–ï–õ 6: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
# ============================================================================
print("\n" + "="*70)
print("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
print("="*70)

print("\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
print(f"  ‚Ä¢ –≠–ø–æ—Ö–∏: 100")
print(f"  ‚Ä¢ Batch Size: 32")
print(f"  ‚Ä¢ Learning Rate: 0.001")
print(f"  ‚Ä¢ Sequence Length: {sequence_length}")
print(f"  ‚Ä¢ Features: {n_features}")
print(f"  ‚Ä¢ –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(X_train)}")
print(f"  ‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(X_val)}")
print(f"  ‚Ä¢ –¢–µ—Å—Ç–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(X_test)}")

print("\n–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...\n")

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    batch_size=32,
    callbacks=callbacks,
    verbose=1,
    shuffle=True
)

print("\n‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

# ============================================================================
# –†–ê–ó–î–ï–õ 7: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –û–ë–£–ß–ï–ù–ò–Ø
# ============================================================================
print("\n" + "="*70)
print("–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –û–ë–£–ß–ï–ù–ò–Ø")
print("="*70)

# –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
plt.style.use('seaborn-v0_8-darkgrid')
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Loss
axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2, color='blue')
axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='red')
axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Epoch', fontsize=12)
axes[0, 0].set_ylabel('Loss', fontsize=12)
axes[0, 0].legend(loc='upper right')
axes[0, 0].grid(True, alpha=0.3)

# 2. Accuracy
axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='blue')
axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='red')
axes[0, 1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Epoch', fontsize=12)
axes[0, 1].set_ylabel('Accuracy', fontsize=12)
axes[0, 1].legend(loc='lower right')
axes[0, 1].grid(True, alpha=0.3)
axes[0, 1].set_ylim([0.5, 1.05])

# 3. Learning Rate (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ –∏—Å—Ç–æ—Ä–∏–∏)
if 'lr' in history.history:
    axes[1, 0].plot(history.history['lr'], label='Learning Rate', linewidth=2, color='green')
    axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
    axes[1, 0].set_xlabel('Epoch', fontsize=12)
    axes[1, 0].set_ylabel('Learning Rate', fontsize=12)
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].set_yscale('log')
else:
    # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ learning rate, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞–∑–Ω–∏—Ü—É –≤ accuracy
    train_acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    axes[1, 0].plot(train_acc, label='Training Accuracy', linewidth=2, color='blue')
    axes[1, 0].plot(val_acc, label='Validation Accuracy', linewidth=2, color='red')
    axes[1, 0].set_title('Accuracy Progression', fontsize=14, fontweight='bold')
    axes[1, 0].set_xlabel('Epoch', fontsize=12)
    axes[1, 0].set_ylabel('Accuracy', fontsize=12)
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

# 4. Accuracy —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–æ–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
epochs = range(1, len(train_acc) + 1)

axes[1, 1].plot(epochs, train_acc, label='Training Accuracy', linewidth=2.5, color='blue')
axes[1, 1].plot(epochs, val_acc, label='Validation Accuracy', linewidth=2.5, color='red')
axes[1, 1].fill_between(epochs, train_acc, val_acc, alpha=0.2, color='gray')
axes[1, 1].set_title('Training vs Validation Accuracy Gap', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('Epoch', fontsize=12)
axes[1, 1].set_ylabel('Accuracy', fontsize=12)
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_history_detailed.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω—ã –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω—ã")

# ============================================================================
# –†–ê–ó–î–ï–õ 8: –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò –ù–ê –¢–ï–°–¢–û–í–û–ú –ù–ê–ë–û–†–ï
# ============================================================================
print("\n" + "="*70)
print("–û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò –ù–ê –¢–ï–°–¢–û–í–û–ú –ù–ê–ë–û–†–ï")
print("="*70)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
try:
    model = keras.models.load_model('best_lstm_fitness_model.h5')
    print("‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
except:
    print("‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å")

# –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
test_results = model.evaluate(X_test, y_test, verbose=0)
test_loss = test_results[0]
test_accuracy = test_results[1]

print("\n" + "="*70)
print("–ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
print("="*70)
print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ:    {history.history['accuracy'][-1]*100:.2f}%")
print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:     {history.history['val_accuracy'][-1]*100:.2f}%")
print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ:         {test_accuracy*100:.2f}%")
print(f"Loss –Ω–∞ —Ç–µ—Å—Ç–µ:             {test_loss:.4f}")
print("="*70)

if test_accuracy >= 0.92:
    print("\n‚úÖ –¢–†–ï–ë–û–í–ê–ù–ò–ï –í–´–ü–û–õ–ù–ï–ù–û! –¢–æ—á–Ω–æ—Å—Ç—å ‚â• 92%")
    print(f"   –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ç–æ—á–Ω–æ—Å—Ç—å: {test_accuracy*100:.2f}%")
    print(f"   –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è: +{(test_accuracy - 0.92)*100:.2f}%")
else:
    print(f"\n‚ö†Ô∏è –¢—Ä–µ–±—É–µ–º–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞")
    print(f"   –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞: {test_accuracy*100:.2f}%")
    print(f"   –¢—Ä–µ–±—É–µ—Ç—Å—è: 92.00%")

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred_probs = model.predict(X_test, verbose=0)
y_pred = np.argmax(y_pred_probs, axis=1)

# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Beginner', 'Intermediate', 'Advanced'],
            yticklabels=['Beginner', 'Intermediate', 'Advanced'],
            cbar_kws={'label': 'Count'})
plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
print(cm)

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
print("\n" + "="*70)
print("–ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–û–ù–ù–´–ô –û–¢–ß–ï–¢")
print("="*70)
print(classification_report(y_test, y_pred,
                          target_names=['Beginner', 'Intermediate', 'Advanced'],
                          digits=4))

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
from sklearn.metrics import precision_recall_fscore_support

precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average=None)

print("\n–î–µ—Ç–∞–ª–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
for i, class_name in enumerate(['Beginner', 'Intermediate', 'Advanced']):
    print(f"\n{class_name}:")
    print(f"  Precision: {precision[i]:.4f}")
    print(f"  Recall:    {recall[i]:.4f}")
    print(f"  F1-Score:  {f1[i]:.4f}")
    print(f"  Support:   {support[i]}")

# ============================================================================
# –†–ê–ó–î–ï–õ 9: –ü–†–ò–ú–ï–†–´ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô
# ============================================================================
print("\n" + "="*70)
print("–ü–†–ò–ú–ï–†–´ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô")
print("="*70)

def predict_with_confidence(model, X_sample, true_label=None):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –≤—ã–≤–æ–¥–æ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
    predictions = model.predict(X_sample[np.newaxis, :], verbose=0)[0]
    predicted_class = np.argmax(predictions)
    confidence = predictions[predicted_class]

    class_names = ['Beginner', 'Intermediate', 'Advanced']

    if true_label is not None:
        print(f"–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {class_names[true_label]}")
    print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {class_names[predicted_class]}")
    print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence*100:.2f}%")
    print("\n–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    for i, cls in enumerate(class_names):
        bar_length = int(predictions[i] * 20)
        bar = '‚ñà' * bar_length
        print(f"  {cls:15s}: {predictions[i]*100:6.2f}% {bar}")

    return predicted_class, confidence

# –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
print("\n–ü—Ä–∏–º–µ—Ä 1 (–ø–µ—Ä–≤—ã–π –æ–±—Ä–∞–∑–µ—Ü):")
print("-" * 50)
predict_with_confidence(model, X_test[0], y_test[0])

print("\n\n–ü—Ä–∏–º–µ—Ä 2 (—Å–ª—É—á–∞–π–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü):")
print("-" * 50)
random_idx = np.random.randint(len(X_test))
predict_with_confidence(model, X_test[random_idx], y_test[random_idx])

print("\n\n–ü—Ä–∏–º–µ—Ä 3 (–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è):")
print("-" * 50)
correct_indices = np.where(y_pred == y_test)[0]
if len(correct_indices) > 0:
    correct_idx = correct_indices[0]
    predict_with_confidence(model, X_test[correct_idx], y_test[correct_idx])

print("\n\n–ü—Ä–∏–º–µ—Ä 4 (–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è):")
print("-" * 50)
wrong_indices = np.where(y_pred != y_test)[0]
if len(wrong_indices) > 0:
    wrong_idx = wrong_indices[0]
    predict_with_confidence(model, X_test[wrong_idx], y_test[wrong_idx])

# ============================================================================
# –†–ê–ó–î–ï–õ 10: –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í
# ============================================================================
print("\n" + "="*70)
print("–ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í")
print("="*70)

try:
    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å–∞ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è
    lstm_layer = model.get_layer('LSTM_1')
    weights, biases = lstm_layer.get_weights()

    # –ê–Ω–∞–ª–∏–∑ –≤—Ö–æ–¥–Ω—ã—Ö –≤–µ—Å–æ–≤
    input_weights = weights[:n_features, :]  # –í–µ—Å–∞ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = np.mean(np.abs(input_weights), axis=1)

    # –ù–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_names = [col for col in df.columns if col != 'Experience_Level']

    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': feature_importance
    }).sort_values('Importance', ascending=True)

    plt.figure(figsize=(10, 6))
    colors = plt.cm.viridis(np.linspace(0.3, 1, len(importance_df)))
    plt.barh(range(len(importance_df)), importance_df['Importance'], color=colors)
    plt.yticks(range(len(importance_df)), importance_df['Feature'])
    plt.xlabel('Average Absolute Weight', fontsize=12)
    plt.title('Feature Importance in LSTM Model', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3, axis='x')
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')
    plt.show()

    print("\n–¢–æ–ø-5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    print(importance_df.tail(5).to_string(index=False))

except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

# ============================================================================
# –†–ê–ó–î–ï–õ 11: –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
# ============================================================================
print("\n" + "="*70)
print("–°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("="*70)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model.save('final_lstm_fitness_model.h5')
print("‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'final_lstm_fitness_model.h5'")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
history_df = pd.DataFrame(history.history)
history_df['epoch'] = range(1, len(history_df) + 1)
history_df.to_csv('training_history.csv', index=False)
print("‚úì –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'training_history.csv'")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
predictions_df = pd.DataFrame({
    'True_Class': y_test,
    'Predicted_Class': y_pred,
    'Confidence': np.max(y_pred_probs, axis=1)
})
predictions_df['Correct'] = predictions_df['True_Class'] == predictions_df['Predicted_Class']
predictions_df.to_csv('test_predictions.csv', index=False)
print("‚úì –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∫–∞–∫ 'test_predictions.csv'")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ scaler
joblib.dump(scaler, 'fitness_scaler.pkl')
print("‚úì Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ 'fitness_scaler.pkl'")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ label encoders
joblib.dump(label_encoders, 'label_encoders.pkl')
print("‚úì Label encoders —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∫–∞–∫ 'label_encoders.pkl'")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –º–æ–¥–µ–ª–∏
model_info = {
    'sequence_length': sequence_length,
    'n_features': n_features,
    'n_classes': 3,
    'test_accuracy': float(test_accuracy),
    'test_loss': float(test_loss),
    'n_samples': len(df),
    'feature_names': [col for col in df.columns if col != 'Experience_Level']
}

import json
with open('model_info.json', 'w') as f:
    json.dump(model_info, f, indent=2)
print("‚úì –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'model_info.json'")

# ============================================================================
# –†–ê–ó–î–ï–õ 12: –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô –ù–û–í–´–• –î–ê–ù–ù–´–•
# ============================================================================
print("\n" + "="*70)
print("–§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô –ù–û–í–´–• –î–ê–ù–ù–´–•")
print("="*70)

def predict_new_customer_data(customer_data):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —É—Ä–æ–≤–Ω—è –æ–ø—ã—Ç–∞ –Ω–æ–≤–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    -----------
    customer_data : dict –∏–ª–∏ pandas Series
        –î–∞–Ω–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç–∞ (–≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫—Ä–æ–º–µ Experience_Level)

    –í–æ–∑–≤—Ä–∞—Ç:
    --------
    dict
        –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    """
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
    if isinstance(customer_data, dict):
        customer_df = pd.DataFrame([customer_data])
    else:
        customer_df = pd.DataFrame([customer_data])

    # –ö–æ–¥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    for col in categorical_cols:
        if col in customer_df.columns and col in label_encoders:
            try:
                customer_df[col] = label_encoders[col].transform(customer_df[col])
            except:
                # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ encoder, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º–æ–µ —á–∞—Å—Ç–æ–µ
                customer_df[col] = 0

    # –ü—Ä–∏–º–µ–Ω—è–µ–º scaler
    customer_scaled = scaler.transform(customer_df.values)

    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–ø–æ–≤—Ç–æ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã)
    if len(customer_scaled) < sequence_length:
        # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—å—à–µ, —á–µ–º –Ω—É–∂–Ω–æ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        repeated_data = np.tile(customer_scaled, (sequence_length, 1))
        sequence = repeated_data[:sequence_length]
    else:
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ sequence_length —Å—Ç—Ä–æ–∫
        sequence = customer_scaled[:sequence_length]

    # –î–æ–±–∞–≤–ª—è–µ–º dimension –¥–ª—è batch
    sequence = sequence.reshape(1, sequence_length, n_features)

    # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    prediction = model.predict(sequence, verbose=0)[0]
    predicted_class = np.argmax(prediction)
    confidence = prediction[predicted_class]

    class_names = ['Beginner', 'Intermediate', 'Advanced']

    result = {
        'predicted_level': class_names[predicted_class],
        'confidence': float(confidence),
        'probabilities': {
            class_names[i]: float(prediction[i]) for i in range(3)
        }
    }

    return result

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
print("\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ predict_new_customer_data():")
print("-" * 60)

example_customer = {
    'Age': 30,
    'Gender': 'Male',
    'Weight': 75,
    'Height': 175,
    'BMI': 24.5,
    'Workout_Hours': 5,
    'Workout_Type': 'Mixed',
    'Calories_Burned': 500,
    'Heart_Rate': 130,
    'Sleep_Hours': 7,
    'Stress_Level': 3,
    'Diet_Score': 8,
    'Previous_Experience': 3
}

result = predict_new_customer_data(example_customer)
print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å: {result['predicted_level']}")
print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']*100:.2f}%")
print("–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:")
for level, prob in result['probabilities'].items():
    print(f"  {level}: {prob*100:.2f}%")

# ============================================================================
# –†–ê–ó–î–ï–õ 13: –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢
# ============================================================================
print("\n" + "="*70)
print("–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢")
print("="*70)

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
final_epoch = len(history.epoch)
stopped_epoch = early_stopping.stopped_epoch
best_epoch = stopped_epoch - early_stopping.patience if stopped_epoch > 0 else final_epoch

print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ LSTM –ú–û–î–ï–õ–ò                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üìä –î–ê–ù–ù–´–ï:
   ‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {len(df):,}
   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {n_features}
   ‚Ä¢ –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {sequence_length}
   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: 3
   ‚Ä¢ –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {class_counts[0]/len(y):.1%} / {class_counts[1]/len(y):.1%} / {class_counts[2]/len(y):.1%}

üß† –ú–û–î–ï–õ–¨:
   ‚Ä¢ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: LSTM (128 ‚Üí 64) + Dense (64 ‚Üí 32 ‚Üí 3)
   ‚Ä¢ –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è: L1/L2 + Dropout + BatchNorm
   ‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: Adam (lr=0.001)
   ‚Ä¢ Loss —Ñ—É–Ω–∫—Ü–∏—è: Sparse Categorical Crossentropy

üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø:
   ‚Ä¢ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç—å:    {history.history['accuracy'][-1]*100:6.2f}%
   ‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–æ—á–Ω–æ—Å—Ç—å:     {history.history['val_accuracy'][-1]*100:6.2f}%
   ‚Ä¢ –¢–µ—Å—Ç —Ç–æ—á–Ω–æ—Å—Ç—å:          {test_accuracy*100:6.2f}%
   ‚Ä¢ –§–∏–Ω–∞–ª—å–Ω—ã–π Loss:         {test_loss:.4f}
   ‚Ä¢ –û–±—É—á–µ–Ω–æ —ç–ø–æ—Ö:           {final_epoch}
   ‚Ä¢ –õ—É—á—à–∞—è —ç–ø–æ—Ö–∞:           {best_epoch}

‚úÖ –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
   ‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: 92.00%
   ‚Ä¢ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {test_accuracy*100:.2f}%
   ‚Ä¢ –°—Ç–∞—Ç—É—Å: {'‚úÖ –¢–†–ï–ë–û–í–ê–ù–ò–ï –í–´–ü–û–õ–ù–ï–ù–û!' if test_accuracy >= 0.92 else '‚ö†Ô∏è –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ'}

üéØ –ú–ï–¢–†–ò–ö–ò –ü–û –ö–õ–ê–°–°–ê–ú:
   –ö–ª–∞—Å—Å          | Precision | Recall  | F1-Score | Support
   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   Beginner       | {precision[0]:.4f}    | {recall[0]:.4f}  | {f1[0]:.4f}   | {support[0]:3d}
   Intermediate   | {precision[1]:.4f}    | {recall[1]:.4f}  | {f1[1]:.4f}   | {support[1]:3d}
   Advanced       | {precision[2]:.4f}    | {recall[2]:.4f}  | {f1[2]:.4f}   | {support[2]:3d}

üíæ –°–û–•–†–ê–ù–ï–ù–ù–´–ï –§–ê–ô–õ–´:
   1. final_lstm_fitness_model.h5      - –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
   2. best_lstm_fitness_model.h5       - –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
   3. training_history.csv             - –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
   4. test_predictions.csv             - –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
   5. fitness_scaler.pkl               - Scaler
   6. label_encoders.pkl               - Label encoders
   7. model_info.json                  - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
   8. training_history_detailed.png    - –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
   9. confusion_matrix.png             - –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
   10. feature_importance.png          - –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

üöÄ –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
   ‚Ä¢ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —É—Ä–æ–≤–Ω—è –æ–ø—ã—Ç–∞ –Ω–æ–≤—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
   ‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö —Ñ–∏—Ç–Ω–µ—Å-–ø—Ä–æ–≥—Ä–∞–º–º
   ‚Ä¢ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏
   ‚Ä¢ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫

üé® –û–°–û–ë–ï–ù–ù–û–°–¢–ò –†–ï–ê–õ–ò–ó–ê–¶–ò–ò:
   ‚Ä¢ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è (–±–µ–∑ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è)
   ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
   ‚Ä¢ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
   ‚Ä¢ –ì–æ—Ç–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è production –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
     –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è —Ñ–∏—Ç–Ω–µ—Å-—Ç—Ä–µ–∫–∏–Ω–≥–∞
     –¢–æ—á–Ω–æ—Å—Ç—å: {test_accuracy*100:.2f}% | –î–∞—Ç–∞: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
""")

print("\n" + "="*70)
print("–ü–†–û–ì–†–ê–ú–ú–ê –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–ê!")
print("="*70)
print("\n‚úÖ –í—Å–µ –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã:")
print("   ‚úì –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã/—Å–æ–∑–¥–∞–Ω—ã")
print("   ‚úì –ú–æ–¥–µ–ª—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –∏ –æ–±—É—á–µ–Ω–∞")
print("   ‚úì –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
print("   ‚úì –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ–∑–¥–∞–Ω—ã –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω—ã")
print("   ‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
print("   ‚úì –ì–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ production")
print("\nüéØ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ñ–∏—Ç–Ω–µ—Å-–ø—Ä–æ–≥—Ä–∞–º–º!")